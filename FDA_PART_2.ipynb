{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FDA_PART-2",
      "provenance": [],
      "authorship_tag": "ABX9TyOyRZl15R7ASTx/6uMr4G+O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anou26/Empirical-Study-and-Comparison-Project/blob/main/FDA_PART_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Libraries"
      ],
      "metadata": {
        "id": "0TKmxsym5nhu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsUXHpeL5aCm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import percentile\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import tweepy\n",
        "from textblob import TextBlob\n",
        "import re # for regular expressions\n",
        "import pandas as pd \n",
        "pd.set_option(\"display.max_colwidth\", 200) \n",
        "import string\n",
        "import requests\n",
        "import folium\n",
        "from folium import plugins\n",
        "from folium.plugins import HeatMap\n",
        "import nltk # for text manipulation\n",
        "from nltk.stem.porter import *\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag, ne_chunk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer as sid\n",
        "from wordcloud import WordCloud\n",
        "from tqdm import tqdm, notebook\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from tqdm import tqdm\n",
        "from gensim.models.doc2vec import LabeledSentence\n",
        "import gensim\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy import stats \n",
        "from sklearn import metrics \n",
        "from sklearn.metrics import mean_squared_error,mean_absolute_error, make_scorer,classification_report,confusion_matrix,accuracy_score,roc_auc_score,roc_curve\n",
        "from sklearn.model_selection import train_test_split,cross_val_score,KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting and Reviewing our dataset"
      ],
      "metadata": {
        "id": "952Q5RHl5vSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"https://raw.githubusercontent.com/gabrielpreda/covid-19-tweets/master/covid19_tweets.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "GvQPkfNb5zKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing"
      ],
      "metadata": {
        "id": "B12B-ANA51Jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write function for removing @user\n",
        "def remove_pattern(input_txt, pattern):\n",
        "    r = re.findall(pattern, input_txt)\n",
        "    for i in r:\n",
        "        input_txt = re.sub(i,'',input_txt)\n",
        "    return input_txt\n",
        "# create new column with removed @user\n",
        "df['clean_text'] = np.vectorize(remove_pattern)(df['text'], '@[\\w]*')\n",
        "df.head(2)"
      ],
      "metadata": {
        "id": "Ky2hjBh5535U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removed HTTP and URLS from tweets"
      ],
      "metadata": {
        "id": "_Lf7z4US576b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "df['clean_text'] = df['clean_text'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "502mpgwp6AWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove special characters, numbers, punctuations\n",
        "df['clean_text'] = df['clean_text'].str.replace('[^a-zA-Z#]+',' ')"
      ],
      "metadata": {
        "id": "NzXWuOWp6XIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "id": "yYHExrh56Y5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove short words\n",
        "df['clean_text'] = df['clean_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w) > 2]))\n",
        "df.head(2)\n"
      ],
      "metadata": {
        "id": "ctb9B_HO6a33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create new variable tokenized tweet \n",
        "tokenized_tweet = df['clean_text'].apply(lambda x: x.split())\n",
        "df.head(2)"
      ],
      "metadata": {
        "id": "GdDgSkG86eEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()\n",
        "# apply stemmer for tokenized_tweet\n",
        "tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x])\n",
        "df.head(2)"
      ],
      "metadata": {
        "id": "ZFft7uSo6gN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# join tokens into one sentence\n",
        "for i in range(len(tokenized_tweet)):\n",
        "    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
        "# change df['clean_text'] to tokenized_tweet\n",
        "df['clean_text']  = tokenized_tweet\n",
        "df.head(2)"
      ],
      "metadata": {
        "id": "vKc2VNol6jL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting features from cleaned tweets"
      ],
      "metadata": {
        "id": "vIN3ow6i6nCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "ebhtrbQL6ls7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')"
      ],
      "metadata": {
        "id": "cTsCJf2G6uK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_text'].apply(lambda x: [item for item in x if item not in stop])"
      ],
      "metadata": {
        "id": "YyDb8tLc6v-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check and calculate sentiment of tweets"
      ],
      "metadata": {
        "id": "8I-y6JZz6zT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creates a function that determines subjectivity and polarity from the textblob package\n",
        "def getTextSubjectivity(clean_text):\n",
        "    return TextBlob(clean_text).sentiment.subjectivity\n",
        "def getTextPolarity(clean_text):\n",
        "    return TextBlob(clean_text).sentiment.polarity\n",
        "#applies these functions to the dataframe\n",
        "df['Subjectivity'] = df['clean_text'].apply(getTextSubjectivity)\n",
        "df['Polarity'] = df['clean_text'].apply(getTextPolarity)\n",
        "#builds a function to calculate and categorize each tweet as Negative, Neutral, and Positive\n",
        "def getTextAnalysis(a):\n",
        "    if a < 0:\n",
        "        return \"Negative\"\n",
        "    elif a == 0:\n",
        "        return \"Neutral\"\n",
        "    else:\n",
        "        return \"Positive\"\n",
        "#creates another column called Score and applies the function to the dataframe\n",
        "df['Score'] = df['Polarity'].apply(getTextAnalysis)"
      ],
      "metadata": {
        "id": "Wxcbhx9j6yLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df=df[['clean_text','Score']]"
      ],
      "metadata": {
        "id": "VqaLrvQZ66Sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.shape"
      ],
      "metadata": {
        "id": "YQv6L_m168IH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.head(1)"
      ],
      "metadata": {
        "id": "F2gPgXCb69xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=new_df.head(20000)"
      ],
      "metadata": {
        "id": "xLqT1S_P6_iN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SUPPORT VECTOR MACHINE (SVM)**"
      ],
      "metadata": {
        "id": "Mt1tUboM7BrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train,valid = train_test_split(data,test_size = 0.3,random_state=0,stratify = data.Score.values) #stratification means that the train_test_split method returns training and test subsets that have the same proportions of class labels as the input dataset.\n",
        "print(\"train shape : \", train.shape)\n",
        "print(\"valid shape : \", valid.shape)"
      ],
      "metadata": {
        "id": "mM2EHsEq7BYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "stop = list(stopwords.words('english'))\n",
        "vectorizer = CountVectorizer(decode_error = 'replace',stop_words = stop)\n",
        "\n",
        "X_train = vectorizer.fit_transform(train.clean_text.values)\n",
        "X_valid = vectorizer.transform(valid.clean_text.values)\n",
        "\n",
        "y_train = train.Score.values\n",
        "y_valid = valid.Score.values\n",
        "\n",
        "print(\"X_train.shape : \", X_train.shape)\n",
        "print(\"X_valid.shape : \", X_valid.shape)\n",
        "print(\"y_train.shape : \", y_train.shape)\n",
        "print(\"y_valid.shape : \", y_valid.shape)"
      ],
      "metadata": {
        "id": "wYxYDqqp7PZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svc = SVC()\n",
        "\n",
        "svc.fit(X_train, y_train)\n",
        "\n",
        "svc_prediction = svc.predict(X_valid)\n",
        "svc_accuracy = accuracy_score(y_valid,svc_prediction)\n",
        "print(\"Training accuracy Score    : \",svc.score(X_train,y_train))\n",
        "print(\"Validation accuracy Score : \",svc_accuracy )\n",
        "print(classification_report(svc_prediction,y_valid))"
      ],
      "metadata": {
        "id": "nGBFFVFr7RQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix = confusion_matrix(svc_prediction,y_valid)\n",
        "matrix_proportions = np.zeros((3,3))\n",
        "for i in range(0,3):\n",
        "    matrix_proportions[i,:] = confusion_matrix[i,:]/float(confusion_matrix[i,:].sum())\n",
        "sents=['Negative','Neutral','Positive']\n",
        "confusion_df = pd.DataFrame(matrix_proportions, index=sents,columns=sents)\n",
        "plt.figure(figsize=(5,5))\n",
        "sns.heatmap(confusion_df,annot=True,annot_kws={\"size\": 12},cmap='gist_gray_r',cbar=False, square=True,fmt='.2f')\n",
        "plt.ylabel(r'True categories',fontsize=14)\n",
        "plt.xlabel(r'Predicted categories',fontsize=14)\n",
        "plt.tick_params(labelsize=12)"
      ],
      "metadata": {
        "id": "7y71zMVr7TTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **BACK PROPAGATION NEURAL NETWORK**"
      ],
      "metadata": {
        "id": "dCeYerNf7XAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# helps in text preprocessing\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, MinMaxScaler\n",
        "\n",
        "# helps in model building\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import SimpleRNN\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# split data into train and test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "train,valid = train_test_split(data,test_size = 0.2,random_state=0,stratify = data.Score.values)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "stop = list(stopwords.words('english'))\n",
        "vectorizer = CountVectorizer(decode_error = 'replace',stop_words = stop)\n",
        "\n",
        "x_train = vectorizer.fit_transform(train.clean_text.values)\n",
        "x_valid = vectorizer.transform(valid.clean_text.values)\n",
        "\n",
        "y_train = train.Score.values\n",
        "y_valid = valid.Score.values"
      ],
      "metadata": {
        "id": "GKzl1ouJ7Vje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "col=x_train.shape[1]\n",
        "\n",
        "model.add(Dense(1000, activation='relu', input_shape =(col,)))\n",
        "model.add(Dense(1000, activation='relu'))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(10,10,10), max_iter=1000)\n",
        "mlp.fit(x_train, y_train)\n",
        "predictions = mlp.predict(x_valid)\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(classification_report(predictions,y_valid))"
      ],
      "metadata": {
        "id": "51hokN6F7hJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix = confusion_matrix(predictions,y_valid)\n",
        "matrix_proportions = np.zeros((3,3))\n",
        "for i in range(0,3):\n",
        "    matrix_proportions[i,:] = confusion_matrix[i,:]/float(confusion_matrix[i,:].sum())\n",
        "sents=['Negative','Neutral','Positive']\n",
        "confusion_df = pd.DataFrame(matrix_proportions, index=sents,columns=sents)\n",
        "plt.figure(figsize=(5,5))\n",
        "sns.heatmap(confusion_df,annot=True,annot_kws={\"size\": 12},cmap='Greens',cbar=False, square=True,fmt='.2f')\n",
        "plt.ylabel(r'True categories',fontsize=14)\n",
        "plt.xlabel(r'Predicted categories',fontsize=14)\n",
        "plt.tick_params(labelsize=12)"
      ],
      "metadata": {
        "id": "s310RfVU7juL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LOGISTIC REGRESSION**"
      ],
      "metadata": {
        "id": "VVVP5XIC7nSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train,valid = train_test_split(new_df,test_size = 0.3,random_state=0,stratify = new_df.Score.values) #stratification means that the train_test_split method returns training and test subsets that have the same proportions of class labels as the input dataset.\n",
        "print(\"train shape : \", train.shape)\n",
        "print(\"valid shape : \", valid.shape)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "stop = list(stopwords.words('english'))\n",
        "vectorizer = CountVectorizer(decode_error = 'replace',stop_words = stop)\n",
        "\n",
        "X_train = vectorizer.fit_transform(train.clean_text.values)\n",
        "X_valid = vectorizer.transform(valid.clean_text.values)\n",
        "\n",
        "y_train = train.Score.values\n",
        "y_valid = valid.Score.values\n",
        "\n",
        "print(\"X_train.shape : \", X_train.shape)\n",
        "print(\"X_valid.shape : \", X_valid.shape)\n",
        "print(\"y_train.shape : \", y_train.shape)\n",
        "print(\"y_valid.shape : \", y_valid.shape)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression()\n",
        "\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "logreg_prediction = logreg.predict(X_valid)\n",
        "logreg_accuracy = accuracy_score(y_valid,logreg_prediction)\n",
        "print(\"Training accuracy Score    : \",logreg.score(X_train,y_train))\n",
        "print(\"Validation accuracy Score : \",logreg_accuracy )\n",
        "print(classification_report(logreg_prediction,y_valid))\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix = confusion_matrix(logreg_prediction,y_valid)\n",
        "matrix_proportions = np.zeros((3,3))\n",
        "for i in range(0,3):\n",
        "    matrix_proportions[i,:] = confusion_matrix[i,:]/float(confusion_matrix[i,:].sum())\n",
        "sents=['Negative','Neutral','Positive']\n",
        "confusion_df = pd.DataFrame(matrix_proportions, index=sents,columns=sents)\n",
        "plt.figure(figsize=(5,5))\n",
        "sns.heatmap(confusion_df,annot=True,annot_kws={\"size\": 12},cmap='Blues',cbar=False, square=True,fmt='.2f')\n",
        "plt.ylabel(r'True categories',fontsize=14)\n",
        "plt.xlabel(r'Predicted categories',fontsize=14)\n",
        "plt.tick_params(labelsize=12)"
      ],
      "metadata": {
        "id": "YPMHk3Xf7mKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RANDOM FOREST CLASSIFIER**\n"
      ],
      "metadata": {
        "id": "m-xgzOPX74gK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train,valid = train_test_split(data,test_size = 0.3,random_state=0,stratify = data.Score.values) #stratification means that the train_test_split method returns training and test subsets that have the same proportions of class labels as the input dataset.\n",
        "print(\"train shape : \", train.shape)\n",
        "print(\"valid shape : \", valid.shape)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "stop = list(stopwords.words('english'))\n",
        "vectorizer = CountVectorizer(decode_error = 'replace',stop_words = stop)\n",
        "\n",
        "X_train = vectorizer.fit_transform(train.clean_text.values)\n",
        "X_valid = vectorizer.transform(valid.clean_text.values)\n",
        "\n",
        "y_train = train.Score.values\n",
        "y_valid = valid.Score.values\n",
        "\n",
        "print(\"X_train.shape : \", X_train.shape)\n",
        "print(\"X_valid.shape : \", X_valid.shape)\n",
        "print(\"y_train.shape : \", y_train.shape)\n",
        "print(\"y_valid.shape : \", y_valid.shape)\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_clf = RandomForestClassifier()\n",
        "\n",
        "rf_clf.fit(X_train,y_train)\n",
        "\n",
        "rf_prediction = rf_clf.predict(X_valid)\n",
        "rf_accuracy = accuracy_score(y_valid,rf_prediction)\n",
        "print(\"Training accuracy Score    : \",rf_clf.score(X_train,y_train))\n",
        "print(\"Validation accuracy Score : \",rf_accuracy )\n",
        "print(classification_report(rf_prediction,y_valid))"
      ],
      "metadata": {
        "id": "TvBndAk777dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix = confusion_matrix(rf_prediction,y_valid)\n",
        "matrix_proportions = np.zeros((3,3))\n",
        "for i in range(0,3):\n",
        "    matrix_proportions[i,:] = confusion_matrix[i,:]/float(confusion_matrix[i,:].sum())\n",
        "sents=['Negative','Neutral','Positive']\n",
        "confusion_df = pd.DataFrame(matrix_proportions, index=sents,columns=sents)\n",
        "plt.figure(figsize=(5,5))\n",
        "sns.heatmap(confusion_df,annot=True,annot_kws={\"size\": 12},cmap='gist_gray_r',cbar=False, square=True,fmt='.2f')\n",
        "plt.ylabel(r'True categories',fontsize=14)\n",
        "plt.xlabel(r'Predicted categories',fontsize=14)\n",
        "plt.tick_params(labelsize=12)"
      ],
      "metadata": {
        "id": "fLsS1gtm7_Jv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}