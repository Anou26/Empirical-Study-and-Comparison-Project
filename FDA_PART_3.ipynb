{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FDA_PART-3",
      "provenance": [],
      "authorship_tag": "ABX9TyOLcSnXKHliYRIRsqrZNM1Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anou26/Empirical-Study-and-Comparison-Project/blob/main/FDA_PART_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM Model for Sentiment Analysis**"
      ],
      "metadata": {
        "id": "Hty77LVu8Rtb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnkXxXQp8PZo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import percentile\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import tweepy\n",
        "from textblob import TextBlob\n",
        "import re # for regular expressions\n",
        "import pandas as pd \n",
        "pd.set_option(\"display.max_colwidth\", 200) \n",
        "import string\n",
        "import requests\n",
        "import nltk # for text manipulation\n",
        "from nltk.stem.porter import *\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag, ne_chunk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer as sid\n",
        "from tqdm import tqdm, notebook\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from scipy import stats \n",
        "from sklearn import metrics \n",
        "from sklearn.metrics import mean_squared_error,mean_absolute_error, make_scorer,classification_report,confusion_matrix,accuracy_score,roc_auc_score,roc_curve\n",
        "from sklearn.model_selection import train_test_split,cross_val_score,KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"https://raw.githubusercontent.com/gabrielpreda/covid-19-tweets/master/covid19_tweets.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "hX-XUntM8bU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write function for removing @user\n",
        "def remove_pattern(input_txt, pattern):\n",
        "    r = re.findall(pattern, input_txt)\n",
        "    for i in r:\n",
        "        input_txt = re.sub(i,'',input_txt)\n",
        "    return input_txt\n",
        "# create new column with removed @user\n",
        "df['clean_text'] = np.vectorize(remove_pattern)(df['text'], '@[\\w]*')\n",
        "df.head(2)"
      ],
      "metadata": {
        "id": "srZyhVAP8fTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "df['clean_text'] = df['clean_text'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "Zg2wX_c38j-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove special characters, numbers, punctuations\n",
        "df['clean_text'] = df['clean_text'].str.replace('[^a-zA-Z#]+',' ')\n",
        "# remove short words\n",
        "df['clean_text'] = df['clean_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w) > 2]))\n",
        "df.head(2)\n",
        "# create new variable tokenized tweet \n",
        "tokenized_tweet = df['clean_text'].apply(lambda x: x.split())\n",
        "df.head(2)"
      ],
      "metadata": {
        "id": "Ddq7ZpAP8mNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()\n",
        "# apply stemmer for tokenized_tweet\n",
        "tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x])\n",
        "# join tokens into one sentence\n",
        "for i in range(len(tokenized_tweet)):\n",
        "    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
        "# change df['clean_text'] to tokenized_tweet\n",
        "df['clean_text']  = tokenized_tweet\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "xAMyg52y8pe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')"
      ],
      "metadata": {
        "id": "NATn8vua8qWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_text'].apply(lambda x: [item for item in x if item not in stop])"
      ],
      "metadata": {
        "id": "HEW6xfsu8sRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creates a function that determines subjectivity and polarity from the textblob package\n",
        "def getTextSubjectivity(clean_text):\n",
        "    return TextBlob(clean_text).sentiment.subjectivity\n",
        "def getTextPolarity(clean_text):\n",
        "    return TextBlob(clean_text).sentiment.polarity\n",
        "#applies these functions to the dataframe\n",
        "df['Subjectivity'] = df['clean_text'].apply(getTextSubjectivity)\n",
        "df['Polarity'] = df['clean_text'].apply(getTextPolarity)\n",
        "#builds a function to calculate and categorize each tweet as Negative, Neutral, and Positive\n",
        "def getTextAnalysis(a):\n",
        "    if a < 0:\n",
        "        return \"Negative\"\n",
        "    elif a == 0:\n",
        "        return \"Neutral\"\n",
        "    else:\n",
        "        return \"Positive\"\n",
        "#creates another column called Score and applies the function to the dataframe\n",
        "df['Score'] = df['Polarity'].apply(getTextAnalysis)"
      ],
      "metadata": {
        "id": "rhxim_hi8uP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df=df[['clean_text','Score']]"
      ],
      "metadata": {
        "id": "SwZ_zQov8wmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=new_df.head(20000)"
      ],
      "metadata": {
        "id": "oD7lLOWg8yyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM Neural Network using Keras"
      ],
      "metadata": {
        "id": "xI1zeCza8d9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\"\"\"Loading the Dataset\"\"\"\n",
        "\n",
        "#from tensorflow.keras.datasets import imdb\n",
        "\n",
        "\"\"\"### **Data Preprocessing**\"\"\"\n",
        "\n",
        "words=20000\n",
        "max_length=100\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train,valid = train_test_split(data,test_size = 0.2,random_state=0,stratify = data.Score.values)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "stop = list(stopwords.words('english'))\n",
        "vectorizer = CountVectorizer(decode_error = 'replace',stop_words = stop)\n",
        "\n",
        "x_train = vectorizer.fit_transform(train.clean_text.values)\n",
        "x_valid = vectorizer.transform(valid.clean_text.values)\n",
        "\n",
        "y_train = train.Score.values\n",
        "y_valid = valid.Score.values\n",
        "\n",
        "\"\"\"Padding the Text\"\"\"\n",
        "\n",
        "#x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train.todense, maxlen=max_length)\n",
        "\n",
        "#x_valid = tf.keras.preprocessing.sequence.pad_sequences(x_valid.todense, maxlen=max_length)\n",
        "\n",
        "word_size=words\n",
        "word_size\n",
        "\n",
        "embed_size=128\n",
        "\n",
        "\"\"\"### Building a Recurrent Neural Network\"\"\"\n",
        "\n",
        "data_model=tf.keras.Sequential()\n",
        "\n",
        "# Embedding Layer\n",
        "data_model.add(tf.keras.layers.Embedding(word_size, embed_size, input_shape=(x_train.shape[1],)))\n",
        "\n",
        "# LSTM Layer\n",
        "data_model.add(tf.keras.layers.LSTM(units=128, activation='tanh'))\n",
        "\n",
        "# Output Layer\n",
        "data_model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "data_model.summary()\n",
        "\n",
        "\"\"\"#### Compiling the model\"\"\"\n",
        "\n",
        "data_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\"\"\"#### Training the model\"\"\"\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(10, 10, 10), max_iter=1000)\n",
        "mlp.fit(x_train, y_train)\n",
        "predictions = mlp.predict(x_valid)\n",
        "accuracy = accuracy_score(y_valid,predictions)\n",
        "print(\"Training accuracy Score    : \",mlp.score(x_train,y_train))\n",
        "print(\"Validation accuracy Score : \",accuracy )\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(classification_report(y_valid,predictions))"
      ],
      "metadata": {
        "id": "b9NCHnh986A_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix = confusion_matrix(y_valid,predictions)\n",
        "matrix_proportions = np.zeros((3,3))\n",
        "for i in range(0,3):\n",
        "    matrix_proportions[i,:] = confusion_matrix[i,:]/float(confusion_matrix[i,:].sum())\n",
        "names=['Negative','Neutral','Positive']\n",
        "confusion_df = pd.DataFrame(matrix_proportions, index=names,columns=names)\n",
        "plt.figure(figsize=(5,5))\n",
        "sns.heatmap(confusion_df,annot=True,annot_kws={\"size\": 12},cmap='YlOrRd',cbar=False, square=True,fmt='.2f')\n",
        "plt.ylabel(r'True categories',fontsize=14)\n",
        "plt.xlabel(r'Predicted categories',fontsize=14)\n",
        "plt.tick_params(labelsize=12)"
      ],
      "metadata": {
        "id": "klCy7G519BZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.utils import resample\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train,valid = train_test_split(data,test_size = 0.3,random_state=0,stratify = data.Score.values) #stratification means that the train_test_split method returns training and test subsets that have the same proportions of class labels as the input dataset.\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "stop = list(stopwords.words('english'))\n",
        "vectorizer = CountVectorizer(decode_error = 'replace',stop_words = stop)\n",
        "\n",
        "X_train = vectorizer.fit_transform(train.clean_text.values)\n",
        "X_valid = vectorizer.transform(valid.clean_text.values)\n",
        "\n",
        "Y_train = train.Score.values\n",
        "Y_valid = valid.Score.values\n",
        "\n",
        "# model\n",
        "embed_dim = 128\n",
        "lstm_out = 192\n",
        "max_fatures = 2000\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_fatures, embed_dim,input_length = X_train.shape[1]))\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "model.add(LSTM(lstm_out, dropout=0.4, recurrent_dropout=0.4))\n",
        "model.add(Dense(2,activation='softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "print(model.summary())\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(10, 10, 10), max_iter=1000)\n",
        "mlp.fit(X_train, Y_train)\n",
        "prediction = mlp.predict(X_valid)\n",
        "accuracy = accuracy_score(Y_valid,prediction)\n",
        "print(\"Training accuracy Score    : \",mlp.score(X_train,Y_train))\n",
        "print(\"Validation accuracy Score : \",accuracy )\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(classification_report(Y_valid,prediction))"
      ],
      "metadata": {
        "id": "s2f0ms1y9Bhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix = confusion_matrix(Y_valid,predictions)\n",
        "matrix_proportions = np.zeros((3,3))\n",
        "for i in range(0,3):\n",
        "    matrix_proportions[i,:] = confusion_matrix[i,:]/float(confusion_matrix[i,:].sum())\n",
        "names=['Negative','Neutral','Positive']\n",
        "confusion_df = pd.DataFrame(matrix_proportions, index=names,columns=names)\n",
        "plt.figure(figsize=(5,5))\n",
        "sns.heatmap(confusion_df,annot=True,annot_kws={\"size\": 12},cmap='YlOrRd',cbar=False, square=True,fmt='.2f')\n",
        "plt.ylabel(r'True categories',fontsize=14)\n",
        "plt.xlabel(r'Predicted categories',fontsize=14)\n",
        "plt.tick_params(labelsize=12)"
      ],
      "metadata": {
        "id": "rxW4GL5t9Bkp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}